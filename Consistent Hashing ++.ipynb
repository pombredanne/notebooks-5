{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements to Consistent Hashing\n",
    "\n",
    "Normally, [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing) is a little expensive, because each node needs the whole set of keys to know which subset it should be working with.\n",
    "\n",
    "But with a little ingenuity in key design, we can enable a pattern that allows each node to only query the work it needs to do!\n",
    "\n",
    "## How Consistent Hashing Works\n",
    "\n",
    "Consistent hashing works by effectively splitting up a ring into multiple parts, and assigning each node a (more or less) equal share.\n",
    "\n",
    "It does this by having each node put the same number of dots on a circle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "PointNode = namedtuple(\"PointNode\", [\"point\", \"node\"])\n",
    "\n",
    "POINTS_BY_NODE = [\n",
    "    PointNode(0, \"a\"),\n",
    "    PointNode(math.pi / 2, \"b\"),\n",
    "    PointNode(math.pi, \"c\"),\n",
    "    PointNode(math.pi * 3 / 2, \"d'\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively enabling buckets in between the points. In the example above, we can just find the point that is less than the point we're attempting to bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"d'\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "def get_node_for_point(node_by_point, point):\n",
    "    \"\"\" given the node_by_point, return the node that the point belongs to. \"\"\"\n",
    "    as_point_node = PointNode(point, \"_\")\n",
    "    index = bisect.bisect_right(node_by_point, as_point_node)\n",
    "    if index == len(node_by_point):\n",
    "        index = -1\n",
    "    return node_by_point[index].node\n",
    "\n",
    "get_node_for_point(POINTS_BY_NODE, math.pi * 7 / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct our own ring from any arbitrary set of nodes, as long as we have a way to uniquely name on versus the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bisect\n",
    "import math\n",
    "import pprint\n",
    "from collections import namedtuple\n",
    "LENGTH = 2 * math.pi\n",
    "\n",
    "PointNode = namedtuple(\"PointNode\", [\"point\", \"node\"])\n",
    "\n",
    "def _calculate_point_for_node(node, point_num):\n",
    "    \"\"\" return back the point for the node, between 0 and 2 * PI \"\"\"\n",
    "    return hash(node + str(point_num)) % LENGTH\n",
    "\n",
    "def points_for_node(node, num_points):\n",
    "    return [_calculate_point_for_node(node, i) for i in range(num_points)]\n",
    "\n",
    "def get_node_by_point(node_names, num_points):\n",
    "    \"\"\" return a tuple of (point, node), ordering by point \"\"\"\n",
    "    point_by_node = [PointNode(p, n) for n in node_names for p in points_for_node(n, num_points)]\n",
    "    point_by_node.sort()\n",
    "    return point_by_node\n",
    "\n",
    "node_by_point = get_node_by_point([\"a\", \"b\", \"c\", \"d\"], 4)\n",
    "get_node_for_point(node_by_point, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing the Points without all the keys\n",
    "Normaly, consistent hashing requires the one executing the algorithm to be aware of two sets of data:\n",
    "    \n",
    "    1. the identifiers of all the nodes in the cluster\n",
    "    2. the set of keys to assign.\n",
    "    \n",
    "This is because the standard algorithm runs through the list of all keys, and assigns them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [1, 2, 4, 6, 8, 11, 14, 17, 20, 23, 26, 27, 28, 31, 33, 36, 39],\n",
       " 'b': [3, 5, 9, 12, 16, 18, 22, 24, 34, 37],\n",
       " 'c': [10, 25, 29, 32, 35],\n",
       " 'd': [0, 7, 13, 15, 19, 21, 30, 38]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_nodes(node_by_point, items):\n",
    "    key_by_bucket = {}\n",
    "    for i in items:\n",
    "        value = hash(i) % LENGTH\n",
    "        node = get_node_for_point(node_by_point, value)\n",
    "        key_by_bucket.setdefault(node, [])\n",
    "        key_by_bucket[node].append(i)\n",
    "    return key_by_bucket\n",
    "\n",
    "items = list(range(40))\n",
    "assign_nodes(node_by_point, items) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note the lack of even distribution here: as a pseudorandom algorithm, you will end up with some minor uneven distribution. We'll talk about that later.)\n",
    "\n",
    "But getting all keys can be inefficient for larger data sets. What happens when we want to consistently hash against a data set of 1 million points?\n",
    "\n",
    "Consistent hashing requires every node to have the full set of keys. But what if each node could just query for the data that's important to it?\n",
    "\n",
    "There is a way to know what those are. Given all the nodes, we can calculate which ranges each node is responsible for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [(0.7221837052256888, 2.1398018192684205),\n",
       "  (2.8125462380799036, 2.9036464881262134),\n",
       "  (3.939380619805206, 4.844545637049649),\n",
       "  (5.724601884363189, 6.102014634518035)],\n",
       " 'b': [(2.4981986005276724, 2.8125462380799036),\n",
       "  (2.9036464881262134, 3.4668217318932335),\n",
       "  (3.4668217318932335, 3.5793269111334993),\n",
       "  (4.909914260552789, 5.724601884363189)],\n",
       " 'c': [(0.4962708040938111, 0.7089051885000046),\n",
       "  (2.4620542383121276, 2.4981986005276724),\n",
       "  (3.5793269111334993, 3.939380619805206),\n",
       "  (6.102014634518035, 6.169806707365048)],\n",
       " 'd': [(0, 0.4962708040938111),\n",
       "  (0.7089051885000046, 0.7221837052256888),\n",
       "  (2.1398018192684205, 2.4620542383121276),\n",
       "  (4.844545637049649, 4.909914260552789),\n",
       "  (6.169806707365048, 6.283185307179586)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ranges_by_node(node_by_point):\n",
    "    \"\"\" return a Dict[node, List[Tuple[lower_bound, upper_bound]]] for the raw nodes by point \"\"\"\n",
    "    range_by_node = {}\n",
    "    previous_point, previous_node = 0, node_by_point[-1].node\n",
    "    for point, node in node_by_point:\n",
    "        point_range = (previous_point, point)\n",
    "        range_by_node.setdefault(node, [])\n",
    "        range_by_node[node].append(point_range)\n",
    "        previous_point, previous_node = point, node\n",
    "    # we close the loop by one last range to the end of the ring\n",
    "    first_node = node_by_point[0].node\n",
    "    range_by_node[first_node].append((previous_point, LENGTH))\n",
    "    \n",
    "    return range_by_node\n",
    "\n",
    "get_ranges_by_node(node_by_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the ranges this node is responsible for. Now we just need a database that knows how to query these ranges.\n",
    "\n",
    "We can accomplish this by storing the range value in the database itself, and index against that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D6N66HENOM']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bisect\n",
    "import random\n",
    "import string\n",
    "\n",
    "def _calculate_point(value):\n",
    "    return hash(value) % LENGTH\n",
    "\n",
    "def _random_string():\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n",
    "\n",
    "VALUES = [_random_string() for _ in range(100)]\n",
    "DATABASE = {_calculate_point(v): v for v in VALUES}\n",
    "INDEX = sorted(DATABASE.keys())\n",
    "\n",
    "def query_database(index, database, bounds):\n",
    "    lower, upper = bounds\n",
    "    lower_index = bisect.bisect_right(index, lower)\n",
    "    upper_index = bisect.bisect_left(index, upper)\n",
    "    return [database[index[i]] for i in range(lower_index, upper_index)]\n",
    "\n",
    "query_database(INDEX, DATABASE, (0.5, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that point, we can pinpoint and query the specific values that are relevant to our node. We can accomplish this with just the information about the nodes themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98YLIK05FO',\n",
       " 'G65IPQJPXK',\n",
       " 'KMF6NLYDEB',\n",
       " '0RBYONF0XK',\n",
       " '7U8PC79F3V',\n",
       " 'CEOLWMNI3W',\n",
       " 'Y7QNLCAXEO',\n",
       " '3JFM658SZZ',\n",
       " 'AOT371FQGD',\n",
       " 'PVPMM7S75V',\n",
       " 'A89JB63ULD',\n",
       " '4NDV0AWK6U',\n",
       " 'UAVSW4MQBN',\n",
       " 'VBX3JSM3TY',\n",
       " 'T4CW8ASMES',\n",
       " 'TC17WA4A7X',\n",
       " '1PLBQO1Q9N',\n",
       " 'MGM68X168W',\n",
       " 'L21PQREYGF',\n",
       " '316IBN0BHP',\n",
       " 'M05207VFGC',\n",
       " '6MC5TS7OJN',\n",
       " 'I6CH3AXE76',\n",
       " 'J6OXH0UHZL',\n",
       " 'MD5ZXGSQS7',\n",
       " '5XIV9B1CKA',\n",
       " '4WDGYWCA43',\n",
       " 'Z86M8ILNL3',\n",
       " 'ZPGE2WL9PF',\n",
       " 'VLTQKJ44Z3',\n",
       " 'V8D46BOPIH',\n",
       " 'GLDCOECKE3',\n",
       " 'YRVACTQ6LF',\n",
       " 'GQH0ZEIAKJ',\n",
       " 'F11EV0HSP8',\n",
       " 'MLTTRGRVH5',\n",
       " 'QLP8FSLY50',\n",
       " 'BW507S1M1C',\n",
       " 'T9Q46PDYFA',\n",
       " 'EPDNXCGLDX',\n",
       " 'H9CLUQZ35M',\n",
       " 'W1WTBYAWJR',\n",
       " 'XFL30R5CHB',\n",
       " 'FIWLOXG4FE',\n",
       " 'B2F4218G10']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_values_for_node(node_by_point, index, database, node):\n",
    "    range_by_node = get_ranges_by_node(node_by_point)\n",
    "    values = []\n",
    "    for bounds in range_by_node[node]:\n",
    "        values += query_database(index, database, bounds)\n",
    "        \n",
    "    return values\n",
    "    \n",
    "query_values_for_node(node_by_point, INDEX, DATABASE, \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's additional performance benefits that can come from storing the index as it's position on the ring. If your database ensures data locality using the same key (such as DynamoDB's shard key), you can gain the advantage of all of your keys living close to each other on disk. This can make the reads for each node's items even faster. \n",
    "\n",
    "\n",
    "## Bucketing Values Evenly\n",
    "\n",
    "As you may have noted earlier, the buckets themselves are not always even. That depends entirely on the distribution of points: for a random distribution, and a high enough number, we will have an extremely high likelyhood of bucketing evenly.\n",
    "\n",
    "So how man buckets is enough? With the approach explained above, it's important to keep the bucket count low: the lower, the fewer queries that have to be made on the database, and the more performant the query on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
